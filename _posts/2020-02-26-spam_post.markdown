---
layout: post
title:  "Spam Detector"
permalink: /spam_ham/
date:   2019-02-01 12:33
image: ../assets/images/spam.png
categories: jekyll update
---

<a href="https://github.com/GaelGil/algorithms/blob/master/data_stuff/spam_analysis/data_eval.ipynb"> Link to the Jupyter Notebook where I show my work</a>


In this blog post I will be talking about how I created a spam detector. This was my first machine learning project so I was very lost at first but as I learned more it was really cool learning how machine learning is used, ways in which people solve problems, and learning how I could solve my problem.  

 I did not use any libraries but I know they exist and make things easier and I will be talking about in the blog post as well. 


I got the data from the UC Irvine Machine Learning Repository


### What I'll be talking about
In this blog post I will be talking about 

My thought proccess<br>
Thinking about the problem<br>
Trying my first algorithm<br>
Analsying the issue<br>
Solution to the previous issue<br>
Results<br>
Solving this issue using sklearn<br>



### What the issue is
So the problem here is if we have incoming text or emails to a user how can we classify messages as spam or ham (not spam) so that we can prevent the user from getting a spam message. 


### Things to know
Supervised Machine Learning:<br>
So because we have the labels this is a machine learning project. This is something I had no idea about. 

Classification:<br>
In machine learning there are different types of tasks that can be done. Some of them are claissfication tasks such as KNN and naive byes. Both these algorithms aim to classify something into a certain class. An example of this is this project. We are predicting wether a message is spam or ham so we are predicting the class of some data. Another big task in machine learning is predicting a numerical value.   An example fot this is linear regression, 

Jupyter NoteBooks:<br>
Jupyter NoteBooks are a really cool tool that is commomly used in machine learning. They are great for visualazing data, etc. 

What is ham?:<br>
Ham is not spam. Here is an example from the data set of what spam is  . And here is an example of what ham is . 

How dictionaries work: <br>
Dictionaries or hashmaps in programming is a data structure. It's a way in which we can structure data by storing a key (some variable) and it's value (some other variable). Let's say for example 


### Attempt 1 
My first algorithm was a very simple. The idea was that my algorithm would take a in a message which would then be made into a dictionary containing the words of the message as it's keys. Once we had the dictionary we needed to store the spam and ham values of that word so we put a dictionary of values for the words set to 0. This would allow us to track the spam and ham values of each word and check if a message has a lot of spam words or not. For example if the word `free` appeared in the message it would have a larger spam value than a ham value so it would get marked as `spam: 1` and `ham: 0`. Here is a sample run of what it would look like using the sentence `hey are u free tonight`.
Here is our dictionary with our words and keys. 

~~~python

words_dict = {
    'hey': ['spam']: 0, ['ham'] : 0,
    'are': ['spam']: 0, ['ham'] : 0,
    'u': ['spam']: 0, ['ham'] : 0,
    'free': ['spam']: 0, ['ham'] : 0,
    'tonight': ['spam']: 0, ['ham'] : 0,
}

~~~

Since we now have our dictionary the next step is to look up our words in another dictionary that contains words and probabilities of them being spam or not already filled in. This will allow us to compare our message to see what our words probabilities of being spam or ham.


~~~python

# Our dict
words_dict = {
    'hey': ['spam']: 0, ['ham'] : 0,
    'are': ['spam']: 0, ['ham'] : 0,
    'u': ['spam']: 0, ['ham'] : 0,
    'free': ['spam']: 0, ['ham'] : 0,
    'tonight': ['spam']: 0, ['ham'] : 0,
}

# The dictionary we look up our words in
prefilled_dict = {
    'hey': ['spam']: 10, ['ham']: 20,
    'the': ['spam']: 7, ['ham'] : 10,
    'u': ['spam']: 2, ['ham'] : 14,
    'sky': ['spam']: 0, ['ham'] : 12,
    'is': ['spam']: 6, ['ham'] : 10,
    'blue': ['spam']: 0, ['ham'] : 15,
    'free': ['spam']: 15, ['ham']: 4,
    'cat': ['spam']: 0, ['ham']: 12,
    'are': ['spam']: 14, ['ham']: 34,
    'exclusive': ['spam']: 16, ['ham']: 3,
    'tonight' : ['spam']: 9, ['ham']: 30,
}

~~~

Once we have compared our words our dictionary would have the values for each word and would look like this.

~~~python

words_dict = {
    'hey': ['spam']: 10, ['ham']: 20,
    'are': ['spam']: 14, ['ham']: 34,
    'u': ['spam']: 2, ['ham'] : 14,
    'free': ['spam']: 25, ['ham']: 7,
    'tonight' : ['spam']: 9, ['ham']: 30,
}
~~~

Now that we have the values for the words we can check which value is higher. In another function I passed in our dictionary where we would do this. The function would check if the spam value was higher than the ham value, if it was we would add 1 to the list and if not we add 0.So we have list with ones and zeroes. If the list had more 1s than zeroes it was spam. Our current example of `hey are u free tonight` would look like this

~~~python

words_nums = [0, 0, 0, 1, 0]

~~~

Because there are more zeeroes than there are ones this would be marked as spam. This algorithm did really bad on the training data I don't really remeber the scores it got but I know it was really low that I didn't bother using the test data. One of the main issues I had was not having words to compare it to which meant almost everything was set to ham. As I said earlier this algorithm looked at everyword in the message and searched it up in a dictionary with prefilled words and values but that dictionary only had maybe 10 or so words already manually set by me. This wouldn't have been such a bad idea if I had lots and lots of words but the issue with this is I would not know what to set their spam and ham values to. So with this came the idea for my next algorithm. 

### Attempt 2 
After looking into possibilites of solving my issue with my first algorithm I had my new idea. The idea was that first we would set all the data (words) into dictionary form, where every word in the dataset would have its spam and ham value. This new algorithm would go through every messsage and clean the message (which I had not done previosly) by removing punctuationa and making every word lowercase. Doing this is good for many reasons one of them being. Once we have our clean message in a list form we can now input the words it into the dictonary with their values (default to 1). 

~~~python

#####This function is for setting the words into the probability dictionary   
def add_words_to_dict(sms_list:list):
    """
    This function takes in a list as its argument and with
    the items of those list we create a dictonary with the
    words as keys. The value of the key are a dictionary 
    with 2 keys one of them being spam and the other ham. 
    The values to those are the words probability of it
    being spam or ham 
    """
    for i in range(len(sms_list)):
        # select current word
        word = sms_list[i]

        # if word is already there ignore it for now
        if word in PROBS:
            pass

        elif word not in PROBS:
            # if word is not in there set their ham and spam values to 1
            PROBS[word] = {'spam' : int(1), 'ham' : int(1)}

~~~


They way in which I assigned the values to a word was by its label (this is supervised machine learning so we have the labels for each message). The algorithm had a function which would take its label as its argument and the message its self. Depending on the label it would add a point to its label value in the ditcionary. If that is confusing here is the function.


~~~python

##### This function is for assigning spam and ham values to words  
def assign_vals_to_words(label:str, sms_list:list):
    """
    This function takes in a string (label of sms) and the sms 
    as a list. Depending on the label we will add 1 to every 
    word on that list label value.
    """
    spam = 'spam'
    ham = 'ham'
    if label == spam:
        # for all spam sms in our train data sets
        for i in range(len(sms_list)):
            # select word
            word = sms_list[i]
            # add 1 to their spam values
            PROBS[word][spam] += 1
    elif label == ham:
        # for all ham sms in our train data sets
        for i in range(len(sms_list)):
            # select word
            word = sms_list[i]
            # add 1 to their ham values
            PROBS[word][ham] += 1       
    else:
        pass


~~~

Once we ran that function our dictionay was filled with its values and looked like this. This is also only a sample because the actual dictioanry is much larger. 


~~~python

probabilities = {
{'go': {'spam': 27, 'ham': 205}, 'until': {'spam': 6, 'ham': 20}, 'jurong': {'spam': 1, 'ham': 2}, 'point': {'spam': 1, 'ham': 12}, 'crazy': {'spam': 4, 'ham': 9}, 'available': {'spam': 4, 'ham': 14}, 'only': {'spam': 68, 'ham': 109}, 'in': {'spam': 66, 'ham': 673}, 'bugis': {'spam': 1, 'ham': 7}, 'n': {'spam': 25, 'ham': 126}, 'great': {'spam': 7, 'ham': 88}, 'world': {'spam': 2, 'ham': 33}, 'la': {'spam': 4, 'ham': 6}, 'e': {'spam': 14, 'ham': 71}, 'buffet': {'spam': 1, 'ham': 3}, 'cine': {'spam': 1, 'ham': 8}, 'there': {'spam': 13, 'ham': 156}, 'got': {'spam': 5, 'ham': 199}, 'amore': {'spam': 1, 'ham': 2}, 'wat': {'spam': 1, 'ham': 77}, 'ok': {'spam': 6, 'ham': 225}, 'lar': {'spam': 1, 'ham': 34}, 'joking': {'spam': 1, 'ham': 4}, 'wif': {'spam': 1, 'ham': 23}, 'u': {'spam': 149, 'ham': 820}, 'oni': {'spam': 1, 'ham': 4}, 'free': {'spam': 186, 'ham': 48}, 'entry': {'spam': 22, 'ham': 1}, 'a': {'spam': 316, 'ham': 886}, 'wkly': {'spam': 10, 'ham': 1}, 'comp': {'spam': 8, 'ham': 3}, 'to': {'spam': 567, 'ham': 1286}, 'win': {'spam': 62, 'ham': 10}, 'fa': {'spam': 5, 'ham': 1}, 'cup': {'spam': 6, 'ham': 5}, 'final': {'spam': 15, 'ham': 3}, 'tkts': {'spam': 5, 'ham': 1}, 'st': {'spam': 27, 'ham': 18}, 'may': {'spam': 8, 'ham': 38}, 'text': {'spam': 100, 'ham': 61}, 'receive': {'spam': 25, 'ham': 6}, 'question': {'spam': 6, 'ham': 15}, 'std': {'spam': 10, 'ham': 1}, 'txt': {'spam': 139, 'ham': 10}, 'rate': {'spam': 25, 'ham': 3}, "t&c's": {'spam': 9, 'ham': 1}, 'apply': {'spam': 26, 'ham': 3}, 'over': {'spam': 15, 'ham': 43}, "'s": {'spam': 7, 'ham': 3}, 'dun': {'spam': 1, 'ham': 45}, 'say': {'spam': 2, 'ham': 82}, 'so': {'spam': 23, 'ham': 352}, 'early': {'spam': 1, 'ham': 31}, 'hor': {'spam': 1, 'ham': 3}, 'c': {'spam': 26, 'ham': 58}, 'already': {'spam': 2, 'ham': 64}, 'then': {'spam': 10, 'ham': 195}, 'nah': {'spam': 1, 'ham': 11}, 'i': {'spam': 47, 'ham': 1904}, "don't": {'spam': 11, 'ham': 113}, 'think': {'spam': 3, 'ham': 107}, 'he': {'spam': 1, 'ham': 164}, 'goes': {'spam': 1, 'ham': 25}, 'usf': {'spam': 1, 'ham': 9}, 'lives': {'spam': 1, 'ham': 5}, 'around': {'spam': 3, 'ham': 48}, 'here': {'spam': 7, 'ham': 94}, 'though': {'spam': 2, 'ham': 22}, 'freemsg': {'spam': 13, 'ham': 1}, 'hey': {'spam': 6, 'ham': 85}, 'darling': {'spam': 3, 'ham': 3}, "it's": {'spam': 4, 'ham': 74}, 'been': {'spam': 38, 'ham': 74}, "week's": {'spam': 5, 'ham': 1}, 'now': {'spam': 176, 'ham': 217}, 'and': {'spam': 114, 'ham': 721}, 'no': {'spam': 58, 'ham': 250}, 'word': {'spam': 24, 'ham': 13}, 'back': {'spam': 22, 'ham': 94}, "i'd": {'spam': 2, 'ham': 9}, 'like': {'spam': 10, 'ham': 194}, 'some': {'spam': 7, 'ham': 93}, 'fun': {'spam': 8, 'ham': 17}, 'you': {'spam': 241, 'ham': 1554}, 'up': {'spam': 20, 'ham': 238}, 'for': {'spam': 161, 'ham': 419}, 'it': {'spam': 24, 'ham': 475}, 'still?': {'spam': 2, 'ham': 2}, 'tb': {'spam': 2, 'ham': 3}, 'xxx': {'spam': 12, 'ham': 20}, 'chgs': {'spam': 2, 'ham': 1}, 'send': {'spam': 62, 'ham': 112}, 'Â£': {'spam': 241, 'ham': 5}, 'rcv': {'spam': 5, 'ham': 1}, 'even': {'spam': 6, 'ham': 50}, 'my': {'spam': 12, 'ham': 625}, 'brother': {'spam': 2, 'ham': 14}, 'is': {'spam': 129, 'ham': 614}, 'not': {'spam': 25, 'ham': 349}, 'speak': {'spam': 9, 'ham': 22}, 'with': {'spam': 90, 'ham': 220}, 'me': {'spam': 27, 'ham': 625}, 'they': {'spam': 16, 'ham': 99}, 'treat': {'spam': 1, 'ham': 14}, 'aids': {'spam': 1, 'ham': 2}, 'patent': {'spam': 1, 'ham': 2}, 'as': {'spam': 31, 'ham': 127}, 'per': {'spam': 40, 'ham': 14}, 'your': {'spam': 206, 'ham': 340}, 'request': {'spam': 2, 'ham': 7}, "'melle": {'spam': 1, 'ham': 4}, 'melle': {'spam': 1, 'ham': 4}, 'oru': {'spam': 1, 'ham': 5}, 'minnaminunginte': {'spam': 1, 'ham': 4}, 'nurungu': {'spam': 1, 'ham': 4}, 'vettam': {'spam': 1, 'ham': 4}, "'": {'spam': 1, 'ham': 11}, 'has': {'spam': 26, 'ham': 68}, 'set': {'spam': 2, 'ham': 17}, 'callertune': {'spam': 1, 'ham': 11}, 'all': {'spam': 27, 'ham': 204}, 'callers': {'spam': 1, 'ham': 6}, 'press': {'spam': 2, 'ham': 11}, 'copy': {'spam': 1, 'ham': 9}, 'friends': {'spam': 5, 'ham': 44}, 'winner': {'spam': 15, 'ham': 1}, 'valued': {'spam': 10, 'ham': 3}, 'network': {'spam': 22, 'ham': 5}, 'customer': {'spam': 41, 'ham': 8}, 'have': {'spam': 111, 'ham': 369}, 'selected': {'spam': 21, 'ham': 3}, 'receivea': {'spam': 3, 'ham': 1}, 'prize': {'spam': 77, 'ham': 1}, 'reward': {'spam': 5, 'ham': 1}, 'claim': {'spam': 91, 'ham': 1}, 'call': {'spam': 296, 'ham': 195}, 'code': {'spam': 21, 'ham': 2}, 'kl': {'spam': 3, 'ham': 1}, 'valid': {'spam': 22, 'ham': 1}, 'hours': {'spam': 5, 'ham': 15}, 'had': {'spam': 11, 'ham': 71}, 'mobile': {'spam': 98, 'ham': 13}, 'months': {'spam': 5, 'ham': 7}, 'or': {'spam': 154, 'ham': 195}, 'more?': {'spam': 3, 'ham': 2}, 'r': {'spam': 27, 'ham': 116}, 'entitled': {'spam': 6, 'ham': 1}, 'update': {'spam': 14, 'ham': 5}, 'the': {'spam': 168, 'ham': 938}, 'latest': {'spam': 25, 'ham': 4}, 'colour': {'spam': 12, 'ham': 5}, 'mobiles': {'spam': 11, 'ham': 1}, 'camera': {'spam': 26, 'ham': 3}, 'co': {'spam': 44, 'ham': 3}, 'on': {'spam': 126, 'ham': 329}, "i'm": {'spam': 9, 'ham': 307}, 'gonna': {'spam': 1, 'ham': 49}, 'be': {'spam': 43, 'ham': 281}, 'home': {'spam': 3, 'ham': 135}, 'soon': {'spam': 2, 'ham': 42}, 'want': {'spam': 26, 'ham': 135}, 'talk': {'spam': 3, 'ham': 33}, 'about': {'spam': 8, 'ham': 133}, 'this': {'spam': 68, 'ham': 211}, 'stuff': {'spam': 2, 'ham': 32}, 'anymore': {'spam': 1, 'ham': 8}, 'tonight': {'spam': 1, 'ham': 30}, 'k?': {'spam': 1, 'ham': 3}, "i've": {'spam': 2, 'ham': 57}, 'cried': {'spam': 1, 'ham': 2}, 'enough': {'spam': 2, 'ham': 20}, 'today': {'spam': 16, 'ham': 101}, 'six': {'spam': 3, 'ham': 2}, 'chances': {'spam': 3, 'ham': 1}, 'cash': {'spam': 60, 'ham': 9}, 'from': {'spam': 101, 'ham': 125}, 'pounds': {'spam': 15, 'ham': 2}, 'txt>': {'spam': 3, 'ham': 1}, 'csh': {'spam': 3, 'ham': 1}, 'cost': {'spam': 24, 'ham': 10}, 'p': {'spam': 151, 'ham': 9}, 'day': {'spam': 27, 'ham': 164}, 'days': {'spam': 12, 'ham': 30}, 'tsandcs': {'spam': 3, 'ham': 1}, 'reply': {'spam': 90, 'ham': 38}, 'hl': {'spam': 16, 'ham': 1}, 'info': {'spam': 16, 'ham': 2}, 'urgent': {'spam': 51, 'ham': 7}, 'won': {'spam': 59, 'ham': 1}, 'week': {'spam': 47, 'ham': 44}, 'membership': {'spam': 2, 'ham': 2}, 'our': {'spam': 67, 'ham': 56}, 'jackpot': {'spam': 2, 'ham': 1}, 't&c': {'spam': 10, 'ham': 1}, 'www': {'spam': 77, 'ham': 2}, 'dbuk': {'spam': 2, 'ham': 1}, 'net': {'spam': 14, 'ham': 9}, 'lccltd': {'spam': 2, 'ham': 1}, 'pobox': {'spam': 37, 'ham': 1}, 'ldnw': {'spam': 4, 'ham': 1}, 'rw': {'spam': 2, 'ham': 1}, 'searching': {'spam': 1, 'ham': 6}, 'right': {'spam': 3, 'ham': 62}, 'words': {'spam': 1, 'ham': 22}, 'thank': {'spam': 2, 'ham': 25}, 'breather': {'spam': 1, 'ham': 2}, 'promise': {'spam': 1, 'ham': 9}, 'wont': {'spam': 1, 'ham': 30}, 'take': {'spam': 13, 'ham': 97}, 'help': {'spam': 17, 'ham': 29}, 'granted': {'spam': 1, 'ham': 2}, 'will': {'spam': 39, 'ham': 277}, 'fulfil': {'spam': 1, 'ham': 2}, 'wonderful': {'spam': 1, 'ham': 13}, 'blessing': {'spam': 1, 'ham': 3}, 'at': {'spam': 26, 'ham': 314}, 'times': {'spam': 1, 'ham': 21}, 'date': {'spam': 10, 'ham': 8}, 'sunday': {'spam': 1, 'ham': 11}, 'xxxmobilemovieclub': {'spam': 3, 'ham': 1}, 'use': {'spam': 7, 'ham': 34}, 'credit': {'spam': 8, 'ham': 5}, 'click': {'spam': 6, 'ham': 2}, 'wap': {'spam': 11, 'ham': 1}, 'link': {'spam': 5, 'ham': 3}, 'next': {'spam': 15, 'ham': 33}, 'message': {'spam': 30, 'ham': 55}, 'here>>': {'spam': 2, 'ham': 1}, 'http': {'spam': 20, 'ham': 1}, 
}

~~~


 Now that we have a dictionary of probabilities we can now try everything again. 


### Testing Accuracy
Before I talk about my other algorithms here is a look at how I tested their acurracy. I created a function called `test_accuracy` this would take in another function (the algorithm) as its arugment and some data (training data). Every algorithm version I tested I imported into the the jupyter notebook and passed in through the `test_accuracy` along with the trainng data. What test acurracy did

~~~python


def get_accuracy(algorithm, data):
    # ammount correct for each label
    correct_ham = 0
    correct_spam = 0
    correct = 0
    
    # the occurances of each label in the data set
    spam = len(data.groupby(['true_category']).get_group('spam'))
    ham = len(data.groupby(['true_category']).get_group('ham'))
    
    # lenght of the dataset
    len_of_data = len(data)

    for i in range(len(data)):
        # get the message and label 
        sms = data['message'][i]
        label = data['true_category'][i]
        
        # call function
        prediction = algorithm(sms) # this returns "spam" or "ham"
        
        if prediction == label:
            # If prediction is correct we add 1 overall correct
            correct += 1
            if prediction == 'spam':
                # If the predicted label is spam and acutal label
                # is spam then we add to amount correct for spam
                correct_spam +=1
            elif prediction =='ham':
                # If the predicted label is ham and acutal label
                # is spam then we add to amount correct for ham
                correct_ham +=1
    
    print((correct_ham+correct_spam)/2)
    print('Correct Ham: ', + correct_ham/ham)
    print('Correct Spam: ', + correct_spam/spam)
    print('Correct: ', + correct/len_of_data)

~~~

### All other attempts with different algorithms
Every algorithm follewd the same proccess as the first one. We sould set the data into dictionary form, count up words and their probabilities and see what we could do with that after.

<!-- naive byes -->
One of my attempts was using naive byes. Naye baies is a 

The equation is given by 


<!-- removing stop words -->
I went online and did some more resaerch to see what other things people did. One of the things that I noticed was people removed stop words. I dont neccisarily know by definitiin what stop words are but from my understanding they are very common word such as `the`, `is`, `they`, etc. So for my next idea to try and improve my algorithms accuracy I removed the stop words. The proccess of the remvoing the stop words was done a long with cleaning the message. In the end this really had no effect and gave me the same results my previous algorithm.
Herer are the results


<!-- graphing -->
Graphed the word probabilites to see if it could show something (final algorithm)

Instead of using some constant like I previously did such as 5 or 10 or 15 to label a message as spam, I could get the average ammount of spam words in a spam message in the data and use that. For example I could have a function that takes in all the spam messages and check how many spam words are in a spam message. I could then take the average and set that as my number. Using a constant could hurt the results by possibly underfiting. 

<!-- using a library -->
Using sklearn


### Challenges/What I learned
One of the challanges I faced were not knowing anything about machine learning. This was my first machine learning project and I had only heard about it but never understood how it wokrs, <!-- how people go about staring sloving their problem -->, I didn't know what supervised and unsupervised ml was, I was just very lost. As I read and did more research I learned a lot tho. This project has thaught me a lot

### future improvements
Not a improvement but while doing research I found out that this same algorithm could possibly work with classifing wether a sentence is positive or negative like if what someone is saying is good or bad. One thing I'd also like to do is make a visual or see what are the most common words in spam message and what are the most common in ham. 